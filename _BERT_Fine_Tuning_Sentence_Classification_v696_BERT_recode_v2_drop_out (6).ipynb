{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEfSbAA4QHas",
    "outputId": "4ef6084c-1b3b-4f8e-b8dd-92422875a242"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 11:52:33.052490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-23 11:52:44.784202: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:52:44.784224: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-23 11:52:45.964777: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-23 11:53:04.455527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:04.456104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:04.456113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 11:53:48.197823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-23 11:53:48.392643: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.393273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.393824: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.394375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.394892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.395427: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.395945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.396440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-10-23 11:53:48.396453: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install wget\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import 696 sentences and set sentences and labels to the 696 data set here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>However , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>If it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>Calgary Stampeders ,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label\n",
       "0       There is manuscript evidence that Austen conti...      1\n",
       "1       In a remarkable comparative analysis , Mandaea...      1\n",
       "2       Before Persephone was released to Hermes , who...      1\n",
       "3       Cogeneration plants are commonly found in dist...      1\n",
       "4       Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1\n",
       "...                                                   ...    ...\n",
       "416763  A Duke Nukem 3D version has been sold for Xbox...      0\n",
       "416764  However , it is becoming replaced as a method ...      0\n",
       "416765  There are hand gestures in both Hindu and Budd...      0\n",
       "416766  If it is necessary to use colors , try to choo...      0\n",
       "416767                               Calgary Stampeders ,      0\n",
       "\n",
       "[416768 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is manuscript evidence that austen conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in a remarkable comparative analysis , mandaea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>before persephone was released to hermes , who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geneva -lrb- , ; , ; , ; ; -rrb- is the second...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>a duke nukem 3d version has been sold for xbox...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>however , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>there are hand gestures in both hindu and budd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>if it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>calgary stampeders ,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_clean  label\n",
       "0       there is manuscript evidence that austen conti...      1\n",
       "1       in a remarkable comparative analysis , mandaea...      1\n",
       "2       before persephone was released to hermes , who...      1\n",
       "3       cogeneration plants are commonly found in dist...      1\n",
       "4       geneva -lrb- , ; , ; , ; ; -rrb- is the second...      1\n",
       "...                                                   ...    ...\n",
       "416763  a duke nukem 3d version has been sold for xbox...      0\n",
       "416764  however , it is becoming replaced as a method ...      0\n",
       "416765  there are hand gestures in both hindu and budd...      0\n",
       "416766  if it is necessary to use colors , try to choo...      0\n",
       "416767                               calgary stampeders ,      0\n",
       "\n",
       "[416768 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is manuscript evidence that austen conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in a remarkable comparative analysis , mandaea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>before persephone was released to hermes , who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cogeneration plants are commonly found in dist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geneva -lrb- , ; , ; , ; ; -rrb- is the second...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416763</th>\n",
       "      <td>a duke nukem 3d version has been sold for xbox...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416764</th>\n",
       "      <td>however , it is becoming replaced as a method ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416765</th>\n",
       "      <td>there are hand gestures in both hindu and budd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416766</th>\n",
       "      <td>if it is necessary to use colors , try to choo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416767</th>\n",
       "      <td>calgary stampeders ,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_clean  label\n",
       "0       there is manuscript evidence that austen conti...      1\n",
       "1       in a remarkable comparative analysis , mandaea...      1\n",
       "2       before persephone was released to hermes , who...      1\n",
       "3       cogeneration plants are commonly found in dist...      1\n",
       "4       geneva -lrb- , ; , ; , ; ; -rrb- is the second...      1\n",
       "...                                                   ...    ...\n",
       "416763  a duke nukem 3d version has been sold for xbox...      0\n",
       "416764  however , it is becoming replaced as a method ...      0\n",
       "416765  there are hand gestures in both hindu and budd...      0\n",
       "416766  if it is necessary to use colors , try to choo...      0\n",
       "416767                               calgary stampeders ,      0\n",
       "\n",
       "[416768 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "# update here set sentences and labels = 696 data instead here:\n",
    "\n",
    "var_WikiText_Train_clean_df = pd.read_csv('data/WikiLarge_Train.csv')\n",
    "\n",
    "display(var_WikiText_Train_clean_df)\n",
    "\n",
    "var_WikiText_Train_clean_df['original_text'] = var_WikiText_Train_clean_df['original_text'].str.lower()\n",
    "var_WikiText_Train_clean_df.columns = ['text_clean','label']\n",
    "\n",
    "\n",
    "display(var_WikiText_Train_clean_df)\n",
    "\n",
    "\n",
    "#var_WikiText_Train_clean_df = pd.read_csv('training_data_export_2_features.csv')\n",
    "\n",
    "var_WikiText_Train_df = var_WikiText_Train_clean_df[['text_clean','label']]\n",
    "\n",
    "\n",
    "#####\n",
    "#####\n",
    "\n",
    "## testing here - only look at first 5000 - 100,000 sentences:\n",
    "#var_WikiText_Train_df = var_WikiText_Train_df.sample(n=100000, random_state=42)\n",
    "#var_WikiText_Train_df = var_WikiText_Train_df.sample(n=100000, random_state=42)\n",
    "display(var_WikiText_Train_df)\n",
    "#display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      " Original:  there is manuscript evidence that austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , anna and james edward austen , made further additions as late as 1814 .\n",
      "Tokenized:  ['there', 'is', 'manuscript', 'evidence', 'that', 'austen', 'continued', 'to', 'work', 'on', 'these', 'pieces', 'as', 'late', 'as', 'the', 'period', '1809', 'a', \"'\", \"'\", '11', ',', 'and', 'that', 'her', 'niece', 'and', 'nephew', ',', 'anna', 'and', 'james', 'edward', 'austen', ',', 'made', 'further', 'additions', 'as', 'late', 'as', '1814', '.']\n",
      "Token IDs:  [2045, 2003, 8356, 3350, 2008, 24177, 2506, 2000, 2147, 2006, 2122, 4109, 2004, 2397, 2004, 1996, 2558, 12861, 1037, 1005, 1005, 2340, 1010, 1998, 2008, 2014, 12286, 1998, 7833, 1010, 4698, 1998, 2508, 3487, 24177, 1010, 2081, 2582, 13134, 2004, 2397, 2004, 10977, 1012]\n"
     ]
    }
   ],
   "source": [
    "# update sentences to be = to 696 sentences here:\n",
    "\n",
    "sentences = var_WikiText_Train_df.text_clean.values\n",
    "labels = var_WikiText_Train_df.label.values\n",
    "\n",
    "\n",
    "# BERT tokenizer - load large vector instance here:\n",
    "print('Loading BERT tokenizer...')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# print some examples here:\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "82ddfcea0e4c4e5a86cf6eca8585be8d",
      "8a256ba4a19e4ec98fe3c3c99fba4daa",
      "8c76faadf2f4415393c6f0a805f0d72b",
      "e0bb735fda99434a90380e7fc664212d",
      "cdb78e75309f4bc09366533331e72431",
      "1058e0b5baa248faa60c1ad146d10bf7",
      "375cc635389c4ddb9bf2aa443df58bae",
      "472198d5b6a748b3a81f9364fd1fa711"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "4e6d97b6-2d4c-42ca-c201-d2b4a88895b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bBdb3pt8LuQ",
    "outputId": "b4d78c6d-0faf-459b-b11a-a26ce40bd32a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pierrel/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  code from Chris McCormick site:\n",
    "#  source:\n",
    "#\n",
    "#  https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences. (was 64 originally here)\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "#print('Original: ', sentences[0])\n",
    "#print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEgLpFVlo1Z-",
    "outputId": "c0ae3d66-6982-4c33-a3f4-ca80e0cd9968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416,763 training samples\n",
      "    5 validation samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "# 696 - pierrel change - try different size training and validation data sets here:\n",
    "# Calculate the number of samples to include in each set.\n",
    "#train_size = int(0.9 * len(dataset))\n",
    "#train_size = int(0.999 * len(dataset))\n",
    "#train_size = int(0.80 * len(dataset))\n",
    "train_size = int(0.99999 * len(dataset))\n",
    "#train_size = int(0.5 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.01, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.01, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.01, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  code from Chris McCormick site:\n",
    "#  source:\n",
    "#\n",
    "#  https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "#batch_size = 32\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    #\"bert-large-uncased\", # Use the 24-layer BERT model, with an uncased vocab.\n",
    "    \n",
    "    #############################################\n",
    "    #############################################\n",
    "    #############################################\n",
    "    #############################################\n",
    "    #  update 10/15/2022 , pierrel - umich\n",
    "    # update use multilingual as the sentences have french, english, german, spanish, and possibly more languages in them\n",
    "    #\"bert-base-multilingual-uncased\", #\n",
    "    \"bert-base-uncased\", #\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    hidden_dropout_prob = 0.01,\n",
    "    attention_probs_dropout_prob = 0.01\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PIiVlDYCtSq",
    "outputId": "7430f38d-de86-4488-bb92-6a9b0142b3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################# - we can remove this code as it is not needed (only informational\n",
    "################################################# - to show the depth of model and layers)\n",
    "\n",
    "#  code from Chris McCormick site:\n",
    "#  source:\n",
    "#\n",
    "#  https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "\n",
    "\n",
    "#optimizer = AdamW(model.parameters(),\n",
    "\n",
    "# change this due to deprecation warning here:\n",
    "# change made 10/15/2022 , pierrel , umich\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "#   696 - pierrel - umich - change the number of epochs to see if it has an effect on accuracy here:\n",
    "#epochs = 4\n",
    "#epochs = 9\n",
    "#epochs = 3\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6J-FYdx6nFE_",
    "outputId": "b2c3e30b-eb5d-4b13-a207-05a48a87ed2a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  6,512.    Elapsed: 0:00:18.\n",
      "  Batch    80  of  6,512.    Elapsed: 0:00:27.\n",
      "  Batch   120  of  6,512.    Elapsed: 0:00:36.\n",
      "  Batch   160  of  6,512.    Elapsed: 0:00:45.\n",
      "  Batch   200  of  6,512.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  6,512.    Elapsed: 0:01:03.\n",
      "  Batch   280  of  6,512.    Elapsed: 0:01:12.\n",
      "  Batch   320  of  6,512.    Elapsed: 0:01:21.\n",
      "  Batch   360  of  6,512.    Elapsed: 0:01:30.\n",
      "  Batch   400  of  6,512.    Elapsed: 0:01:39.\n",
      "  Batch   440  of  6,512.    Elapsed: 0:01:48.\n",
      "  Batch   480  of  6,512.    Elapsed: 0:01:57.\n",
      "  Batch   520  of  6,512.    Elapsed: 0:02:06.\n",
      "  Batch   560  of  6,512.    Elapsed: 0:02:15.\n",
      "  Batch   600  of  6,512.    Elapsed: 0:02:24.\n",
      "  Batch   640  of  6,512.    Elapsed: 0:02:33.\n",
      "  Batch   680  of  6,512.    Elapsed: 0:02:42.\n",
      "  Batch   720  of  6,512.    Elapsed: 0:02:51.\n",
      "  Batch   760  of  6,512.    Elapsed: 0:03:00.\n",
      "  Batch   800  of  6,512.    Elapsed: 0:03:09.\n",
      "  Batch   840  of  6,512.    Elapsed: 0:03:18.\n",
      "  Batch   880  of  6,512.    Elapsed: 0:03:27.\n",
      "  Batch   920  of  6,512.    Elapsed: 0:03:36.\n",
      "  Batch   960  of  6,512.    Elapsed: 0:03:45.\n",
      "  Batch 1,000  of  6,512.    Elapsed: 0:03:54.\n",
      "  Batch 1,040  of  6,512.    Elapsed: 0:04:03.\n",
      "  Batch 1,080  of  6,512.    Elapsed: 0:04:12.\n",
      "  Batch 1,120  of  6,512.    Elapsed: 0:04:21.\n",
      "  Batch 1,160  of  6,512.    Elapsed: 0:04:30.\n",
      "  Batch 1,200  of  6,512.    Elapsed: 0:04:39.\n",
      "  Batch 1,240  of  6,512.    Elapsed: 0:04:48.\n",
      "  Batch 1,280  of  6,512.    Elapsed: 0:04:57.\n",
      "  Batch 1,320  of  6,512.    Elapsed: 0:05:06.\n",
      "  Batch 1,360  of  6,512.    Elapsed: 0:05:15.\n",
      "  Batch 1,400  of  6,512.    Elapsed: 0:05:24.\n",
      "  Batch 1,440  of  6,512.    Elapsed: 0:05:33.\n",
      "  Batch 1,480  of  6,512.    Elapsed: 0:05:42.\n",
      "  Batch 1,520  of  6,512.    Elapsed: 0:05:51.\n",
      "  Batch 1,560  of  6,512.    Elapsed: 0:06:00.\n",
      "  Batch 1,600  of  6,512.    Elapsed: 0:06:09.\n",
      "  Batch 1,640  of  6,512.    Elapsed: 0:06:18.\n",
      "  Batch 1,680  of  6,512.    Elapsed: 0:06:27.\n",
      "  Batch 1,720  of  6,512.    Elapsed: 0:06:36.\n",
      "  Batch 1,760  of  6,512.    Elapsed: 0:06:45.\n",
      "  Batch 1,800  of  6,512.    Elapsed: 0:06:54.\n",
      "  Batch 1,840  of  6,512.    Elapsed: 0:07:03.\n",
      "  Batch 1,880  of  6,512.    Elapsed: 0:07:12.\n",
      "  Batch 1,920  of  6,512.    Elapsed: 0:07:21.\n",
      "  Batch 1,960  of  6,512.    Elapsed: 0:07:30.\n",
      "  Batch 2,000  of  6,512.    Elapsed: 0:07:39.\n",
      "  Batch 2,040  of  6,512.    Elapsed: 0:07:48.\n",
      "  Batch 2,080  of  6,512.    Elapsed: 0:07:57.\n",
      "  Batch 2,120  of  6,512.    Elapsed: 0:08:07.\n",
      "  Batch 2,160  of  6,512.    Elapsed: 0:08:16.\n",
      "  Batch 2,200  of  6,512.    Elapsed: 0:08:25.\n",
      "  Batch 2,240  of  6,512.    Elapsed: 0:08:34.\n",
      "  Batch 2,280  of  6,512.    Elapsed: 0:08:43.\n",
      "  Batch 2,320  of  6,512.    Elapsed: 0:08:52.\n",
      "  Batch 2,360  of  6,512.    Elapsed: 0:09:01.\n",
      "  Batch 2,400  of  6,512.    Elapsed: 0:09:10.\n",
      "  Batch 2,440  of  6,512.    Elapsed: 0:09:19.\n",
      "  Batch 2,480  of  6,512.    Elapsed: 0:09:28.\n",
      "  Batch 2,520  of  6,512.    Elapsed: 0:09:37.\n",
      "  Batch 2,560  of  6,512.    Elapsed: 0:09:46.\n",
      "  Batch 2,600  of  6,512.    Elapsed: 0:09:55.\n",
      "  Batch 2,640  of  6,512.    Elapsed: 0:10:04.\n",
      "  Batch 2,680  of  6,512.    Elapsed: 0:10:13.\n",
      "  Batch 2,720  of  6,512.    Elapsed: 0:10:22.\n",
      "  Batch 2,760  of  6,512.    Elapsed: 0:10:31.\n",
      "  Batch 2,800  of  6,512.    Elapsed: 0:10:40.\n",
      "  Batch 2,840  of  6,512.    Elapsed: 0:10:49.\n",
      "  Batch 2,880  of  6,512.    Elapsed: 0:10:58.\n",
      "  Batch 2,920  of  6,512.    Elapsed: 0:11:07.\n",
      "  Batch 2,960  of  6,512.    Elapsed: 0:11:15.\n",
      "  Batch 3,000  of  6,512.    Elapsed: 0:11:24.\n",
      "  Batch 3,040  of  6,512.    Elapsed: 0:11:33.\n",
      "  Batch 3,080  of  6,512.    Elapsed: 0:11:42.\n",
      "  Batch 3,120  of  6,512.    Elapsed: 0:11:51.\n",
      "  Batch 3,160  of  6,512.    Elapsed: 0:12:00.\n",
      "  Batch 3,200  of  6,512.    Elapsed: 0:12:09.\n",
      "  Batch 3,240  of  6,512.    Elapsed: 0:12:18.\n",
      "  Batch 3,280  of  6,512.    Elapsed: 0:12:27.\n",
      "  Batch 3,320  of  6,512.    Elapsed: 0:12:36.\n",
      "  Batch 3,360  of  6,512.    Elapsed: 0:12:45.\n",
      "  Batch 3,400  of  6,512.    Elapsed: 0:12:54.\n",
      "  Batch 3,440  of  6,512.    Elapsed: 0:13:03.\n",
      "  Batch 3,480  of  6,512.    Elapsed: 0:13:12.\n",
      "  Batch 3,520  of  6,512.    Elapsed: 0:13:21.\n",
      "  Batch 3,560  of  6,512.    Elapsed: 0:13:30.\n",
      "  Batch 3,600  of  6,512.    Elapsed: 0:13:39.\n",
      "  Batch 3,640  of  6,512.    Elapsed: 0:13:48.\n",
      "  Batch 3,680  of  6,512.    Elapsed: 0:13:57.\n",
      "  Batch 3,720  of  6,512.    Elapsed: 0:14:06.\n",
      "  Batch 3,760  of  6,512.    Elapsed: 0:14:15.\n",
      "  Batch 3,800  of  6,512.    Elapsed: 0:14:24.\n",
      "  Batch 3,840  of  6,512.    Elapsed: 0:14:33.\n",
      "  Batch 3,880  of  6,512.    Elapsed: 0:14:42.\n",
      "  Batch 3,920  of  6,512.    Elapsed: 0:14:51.\n",
      "  Batch 3,960  of  6,512.    Elapsed: 0:15:00.\n",
      "  Batch 4,000  of  6,512.    Elapsed: 0:15:09.\n",
      "  Batch 4,040  of  6,512.    Elapsed: 0:15:18.\n",
      "  Batch 4,080  of  6,512.    Elapsed: 0:15:27.\n",
      "  Batch 4,120  of  6,512.    Elapsed: 0:15:36.\n",
      "  Batch 4,160  of  6,512.    Elapsed: 0:15:45.\n",
      "  Batch 4,200  of  6,512.    Elapsed: 0:15:54.\n",
      "  Batch 4,240  of  6,512.    Elapsed: 0:16:03.\n",
      "  Batch 4,280  of  6,512.    Elapsed: 0:16:12.\n",
      "  Batch 4,320  of  6,512.    Elapsed: 0:16:21.\n",
      "  Batch 4,360  of  6,512.    Elapsed: 0:16:29.\n",
      "  Batch 4,400  of  6,512.    Elapsed: 0:16:38.\n",
      "  Batch 4,440  of  6,512.    Elapsed: 0:16:47.\n",
      "  Batch 4,480  of  6,512.    Elapsed: 0:16:56.\n",
      "  Batch 4,520  of  6,512.    Elapsed: 0:17:05.\n",
      "  Batch 4,560  of  6,512.    Elapsed: 0:17:14.\n",
      "  Batch 4,600  of  6,512.    Elapsed: 0:17:23.\n",
      "  Batch 4,640  of  6,512.    Elapsed: 0:17:32.\n",
      "  Batch 4,680  of  6,512.    Elapsed: 0:17:41.\n",
      "  Batch 4,720  of  6,512.    Elapsed: 0:17:50.\n",
      "  Batch 4,760  of  6,512.    Elapsed: 0:17:59.\n",
      "  Batch 4,800  of  6,512.    Elapsed: 0:18:08.\n",
      "  Batch 4,840  of  6,512.    Elapsed: 0:18:17.\n",
      "  Batch 4,880  of  6,512.    Elapsed: 0:18:26.\n",
      "  Batch 4,920  of  6,512.    Elapsed: 0:18:35.\n",
      "  Batch 4,960  of  6,512.    Elapsed: 0:18:44.\n",
      "  Batch 5,000  of  6,512.    Elapsed: 0:18:53.\n",
      "  Batch 5,040  of  6,512.    Elapsed: 0:19:02.\n",
      "  Batch 5,080  of  6,512.    Elapsed: 0:19:11.\n",
      "  Batch 5,120  of  6,512.    Elapsed: 0:19:20.\n",
      "  Batch 5,160  of  6,512.    Elapsed: 0:19:29.\n",
      "  Batch 5,200  of  6,512.    Elapsed: 0:19:38.\n",
      "  Batch 5,240  of  6,512.    Elapsed: 0:19:47.\n",
      "  Batch 5,280  of  6,512.    Elapsed: 0:19:56.\n",
      "  Batch 5,320  of  6,512.    Elapsed: 0:20:05.\n",
      "  Batch 5,360  of  6,512.    Elapsed: 0:20:14.\n",
      "  Batch 5,400  of  6,512.    Elapsed: 0:20:23.\n",
      "  Batch 5,440  of  6,512.    Elapsed: 0:20:32.\n",
      "  Batch 5,480  of  6,512.    Elapsed: 0:20:41.\n",
      "  Batch 5,520  of  6,512.    Elapsed: 0:20:50.\n",
      "  Batch 5,560  of  6,512.    Elapsed: 0:20:59.\n",
      "  Batch 5,600  of  6,512.    Elapsed: 0:21:08.\n",
      "  Batch 5,640  of  6,512.    Elapsed: 0:21:17.\n",
      "  Batch 5,680  of  6,512.    Elapsed: 0:21:26.\n",
      "  Batch 5,720  of  6,512.    Elapsed: 0:21:35.\n",
      "  Batch 5,760  of  6,512.    Elapsed: 0:21:44.\n",
      "  Batch 5,800  of  6,512.    Elapsed: 0:21:53.\n",
      "  Batch 5,840  of  6,512.    Elapsed: 0:22:02.\n",
      "  Batch 5,880  of  6,512.    Elapsed: 0:22:11.\n",
      "  Batch 5,920  of  6,512.    Elapsed: 0:22:20.\n",
      "  Batch 5,960  of  6,512.    Elapsed: 0:22:29.\n",
      "  Batch 6,000  of  6,512.    Elapsed: 0:22:38.\n",
      "  Batch 6,040  of  6,512.    Elapsed: 0:22:46.\n",
      "  Batch 6,080  of  6,512.    Elapsed: 0:22:55.\n",
      "  Batch 6,120  of  6,512.    Elapsed: 0:23:04.\n",
      "  Batch 6,160  of  6,512.    Elapsed: 0:23:13.\n",
      "  Batch 6,200  of  6,512.    Elapsed: 0:23:22.\n",
      "  Batch 6,240  of  6,512.    Elapsed: 0:23:31.\n",
      "  Batch 6,280  of  6,512.    Elapsed: 0:23:40.\n",
      "  Batch 6,320  of  6,512.    Elapsed: 0:23:49.\n",
      "  Batch 6,360  of  6,512.    Elapsed: 0:23:58.\n",
      "  Batch 6,400  of  6,512.    Elapsed: 0:24:07.\n",
      "  Batch 6,440  of  6,512.    Elapsed: 0:24:16.\n",
      "  Batch 6,480  of  6,512.    Elapsed: 0:24:25.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epoch took: 0:24:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.27\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  6,512.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  6,512.    Elapsed: 0:00:18.\n",
      "  Batch   120  of  6,512.    Elapsed: 0:00:27.\n",
      "  Batch   160  of  6,512.    Elapsed: 0:00:36.\n",
      "  Batch   200  of  6,512.    Elapsed: 0:00:45.\n",
      "  Batch   240  of  6,512.    Elapsed: 0:00:54.\n",
      "  Batch   280  of  6,512.    Elapsed: 0:01:03.\n",
      "  Batch   320  of  6,512.    Elapsed: 0:01:12.\n",
      "  Batch   360  of  6,512.    Elapsed: 0:01:21.\n",
      "  Batch   400  of  6,512.    Elapsed: 0:01:30.\n",
      "  Batch   440  of  6,512.    Elapsed: 0:01:39.\n",
      "  Batch   480  of  6,512.    Elapsed: 0:01:48.\n",
      "  Batch   520  of  6,512.    Elapsed: 0:01:57.\n",
      "  Batch   560  of  6,512.    Elapsed: 0:02:06.\n",
      "  Batch   600  of  6,512.    Elapsed: 0:02:15.\n",
      "  Batch   640  of  6,512.    Elapsed: 0:02:24.\n",
      "  Batch   680  of  6,512.    Elapsed: 0:02:33.\n",
      "  Batch   720  of  6,512.    Elapsed: 0:02:42.\n",
      "  Batch   760  of  6,512.    Elapsed: 0:02:50.\n",
      "  Batch   800  of  6,512.    Elapsed: 0:02:59.\n",
      "  Batch   840  of  6,512.    Elapsed: 0:03:08.\n",
      "  Batch   880  of  6,512.    Elapsed: 0:03:17.\n",
      "  Batch   920  of  6,512.    Elapsed: 0:03:26.\n",
      "  Batch   960  of  6,512.    Elapsed: 0:03:35.\n",
      "  Batch 1,000  of  6,512.    Elapsed: 0:03:44.\n",
      "  Batch 1,040  of  6,512.    Elapsed: 0:03:53.\n",
      "  Batch 1,080  of  6,512.    Elapsed: 0:04:02.\n",
      "  Batch 1,120  of  6,512.    Elapsed: 0:04:11.\n",
      "  Batch 1,160  of  6,512.    Elapsed: 0:04:20.\n",
      "  Batch 1,200  of  6,512.    Elapsed: 0:04:29.\n",
      "  Batch 1,240  of  6,512.    Elapsed: 0:04:38.\n",
      "  Batch 1,280  of  6,512.    Elapsed: 0:04:47.\n",
      "  Batch 1,320  of  6,512.    Elapsed: 0:04:56.\n",
      "  Batch 1,360  of  6,512.    Elapsed: 0:05:05.\n",
      "  Batch 1,400  of  6,512.    Elapsed: 0:05:14.\n",
      "  Batch 1,440  of  6,512.    Elapsed: 0:05:23.\n",
      "  Batch 1,480  of  6,512.    Elapsed: 0:05:32.\n",
      "  Batch 1,520  of  6,512.    Elapsed: 0:05:41.\n",
      "  Batch 1,560  of  6,512.    Elapsed: 0:05:50.\n",
      "  Batch 1,600  of  6,512.    Elapsed: 0:05:59.\n",
      "  Batch 1,640  of  6,512.    Elapsed: 0:06:08.\n",
      "  Batch 1,680  of  6,512.    Elapsed: 0:06:17.\n",
      "  Batch 1,720  of  6,512.    Elapsed: 0:06:26.\n",
      "  Batch 1,760  of  6,512.    Elapsed: 0:06:35.\n",
      "  Batch 1,800  of  6,512.    Elapsed: 0:06:44.\n",
      "  Batch 1,840  of  6,512.    Elapsed: 0:06:53.\n",
      "  Batch 1,880  of  6,512.    Elapsed: 0:07:02.\n",
      "  Batch 1,920  of  6,512.    Elapsed: 0:07:11.\n",
      "  Batch 1,960  of  6,512.    Elapsed: 0:07:20.\n",
      "  Batch 2,000  of  6,512.    Elapsed: 0:07:29.\n",
      "  Batch 2,040  of  6,512.    Elapsed: 0:07:38.\n",
      "  Batch 2,080  of  6,512.    Elapsed: 0:07:47.\n",
      "  Batch 2,120  of  6,512.    Elapsed: 0:07:56.\n",
      "  Batch 2,160  of  6,512.    Elapsed: 0:08:05.\n",
      "  Batch 2,200  of  6,512.    Elapsed: 0:08:14.\n",
      "  Batch 2,240  of  6,512.    Elapsed: 0:08:23.\n",
      "  Batch 2,280  of  6,512.    Elapsed: 0:08:32.\n",
      "  Batch 2,320  of  6,512.    Elapsed: 0:08:41.\n",
      "  Batch 2,360  of  6,512.    Elapsed: 0:08:50.\n",
      "  Batch 2,400  of  6,512.    Elapsed: 0:08:59.\n",
      "  Batch 2,440  of  6,512.    Elapsed: 0:09:08.\n",
      "  Batch 2,480  of  6,512.    Elapsed: 0:09:17.\n",
      "  Batch 2,520  of  6,512.    Elapsed: 0:09:26.\n",
      "  Batch 2,560  of  6,512.    Elapsed: 0:09:35.\n",
      "  Batch 2,600  of  6,512.    Elapsed: 0:09:44.\n",
      "  Batch 2,640  of  6,512.    Elapsed: 0:09:53.\n",
      "  Batch 2,680  of  6,512.    Elapsed: 0:10:02.\n",
      "  Batch 2,720  of  6,512.    Elapsed: 0:10:11.\n",
      "  Batch 2,760  of  6,512.    Elapsed: 0:10:20.\n",
      "  Batch 2,800  of  6,512.    Elapsed: 0:10:29.\n",
      "  Batch 2,840  of  6,512.    Elapsed: 0:10:38.\n",
      "  Batch 2,880  of  6,512.    Elapsed: 0:10:47.\n",
      "  Batch 2,920  of  6,512.    Elapsed: 0:10:56.\n",
      "  Batch 2,960  of  6,512.    Elapsed: 0:11:05.\n",
      "  Batch 3,000  of  6,512.    Elapsed: 0:11:14.\n",
      "  Batch 3,040  of  6,512.    Elapsed: 0:11:23.\n",
      "  Batch 3,080  of  6,512.    Elapsed: 0:11:32.\n",
      "  Batch 3,120  of  6,512.    Elapsed: 0:11:41.\n",
      "  Batch 3,160  of  6,512.    Elapsed: 0:11:50.\n",
      "  Batch 3,200  of  6,512.    Elapsed: 0:11:59.\n",
      "  Batch 3,240  of  6,512.    Elapsed: 0:12:08.\n",
      "  Batch 3,280  of  6,512.    Elapsed: 0:12:17.\n",
      "  Batch 3,320  of  6,512.    Elapsed: 0:12:26.\n",
      "  Batch 3,360  of  6,512.    Elapsed: 0:12:35.\n",
      "  Batch 3,400  of  6,512.    Elapsed: 0:12:44.\n",
      "  Batch 3,440  of  6,512.    Elapsed: 0:12:53.\n",
      "  Batch 3,480  of  6,512.    Elapsed: 0:13:02.\n",
      "  Batch 3,520  of  6,512.    Elapsed: 0:13:11.\n",
      "  Batch 3,560  of  6,512.    Elapsed: 0:13:20.\n",
      "  Batch 3,600  of  6,512.    Elapsed: 0:13:29.\n",
      "  Batch 3,640  of  6,512.    Elapsed: 0:13:38.\n",
      "  Batch 3,680  of  6,512.    Elapsed: 0:13:47.\n",
      "  Batch 3,720  of  6,512.    Elapsed: 0:13:56.\n",
      "  Batch 3,760  of  6,512.    Elapsed: 0:14:05.\n",
      "  Batch 3,800  of  6,512.    Elapsed: 0:14:14.\n",
      "  Batch 3,840  of  6,512.    Elapsed: 0:14:23.\n",
      "  Batch 3,880  of  6,512.    Elapsed: 0:14:32.\n",
      "  Batch 3,920  of  6,512.    Elapsed: 0:14:41.\n",
      "  Batch 3,960  of  6,512.    Elapsed: 0:14:50.\n",
      "  Batch 4,000  of  6,512.    Elapsed: 0:14:59.\n",
      "  Batch 4,040  of  6,512.    Elapsed: 0:15:08.\n",
      "  Batch 4,080  of  6,512.    Elapsed: 0:15:17.\n",
      "  Batch 4,120  of  6,512.    Elapsed: 0:15:26.\n",
      "  Batch 4,160  of  6,512.    Elapsed: 0:15:35.\n",
      "  Batch 4,200  of  6,512.    Elapsed: 0:15:44.\n",
      "  Batch 4,240  of  6,512.    Elapsed: 0:15:53.\n",
      "  Batch 4,280  of  6,512.    Elapsed: 0:16:02.\n",
      "  Batch 4,320  of  6,512.    Elapsed: 0:16:11.\n",
      "  Batch 4,360  of  6,512.    Elapsed: 0:16:20.\n",
      "  Batch 4,400  of  6,512.    Elapsed: 0:16:29.\n",
      "  Batch 4,440  of  6,512.    Elapsed: 0:16:37.\n",
      "  Batch 4,480  of  6,512.    Elapsed: 0:16:46.\n",
      "  Batch 4,520  of  6,512.    Elapsed: 0:16:55.\n",
      "  Batch 4,560  of  6,512.    Elapsed: 0:17:04.\n",
      "  Batch 4,600  of  6,512.    Elapsed: 0:17:13.\n",
      "  Batch 4,640  of  6,512.    Elapsed: 0:17:22.\n",
      "  Batch 4,680  of  6,512.    Elapsed: 0:17:31.\n",
      "  Batch 4,720  of  6,512.    Elapsed: 0:17:40.\n",
      "  Batch 4,760  of  6,512.    Elapsed: 0:17:49.\n",
      "  Batch 4,800  of  6,512.    Elapsed: 0:17:58.\n",
      "  Batch 4,840  of  6,512.    Elapsed: 0:18:07.\n",
      "  Batch 4,880  of  6,512.    Elapsed: 0:18:16.\n",
      "  Batch 4,920  of  6,512.    Elapsed: 0:18:25.\n",
      "  Batch 4,960  of  6,512.    Elapsed: 0:18:34.\n",
      "  Batch 5,000  of  6,512.    Elapsed: 0:18:43.\n",
      "  Batch 5,040  of  6,512.    Elapsed: 0:18:52.\n",
      "  Batch 5,080  of  6,512.    Elapsed: 0:19:01.\n",
      "  Batch 5,120  of  6,512.    Elapsed: 0:19:10.\n",
      "  Batch 5,160  of  6,512.    Elapsed: 0:19:19.\n",
      "  Batch 5,200  of  6,512.    Elapsed: 0:19:28.\n",
      "  Batch 5,240  of  6,512.    Elapsed: 0:19:37.\n",
      "  Batch 5,280  of  6,512.    Elapsed: 0:19:46.\n",
      "  Batch 5,320  of  6,512.    Elapsed: 0:19:55.\n",
      "  Batch 5,360  of  6,512.    Elapsed: 0:20:04.\n",
      "  Batch 5,400  of  6,512.    Elapsed: 0:20:13.\n",
      "  Batch 5,440  of  6,512.    Elapsed: 0:20:22.\n",
      "  Batch 5,480  of  6,512.    Elapsed: 0:20:31.\n",
      "  Batch 5,520  of  6,512.    Elapsed: 0:20:40.\n",
      "  Batch 5,560  of  6,512.    Elapsed: 0:20:49.\n",
      "  Batch 5,600  of  6,512.    Elapsed: 0:20:58.\n",
      "  Batch 5,640  of  6,512.    Elapsed: 0:21:07.\n",
      "  Batch 5,680  of  6,512.    Elapsed: 0:21:16.\n",
      "  Batch 5,720  of  6,512.    Elapsed: 0:21:25.\n",
      "  Batch 5,760  of  6,512.    Elapsed: 0:21:34.\n",
      "  Batch 5,800  of  6,512.    Elapsed: 0:21:43.\n",
      "  Batch 5,840  of  6,512.    Elapsed: 0:21:51.\n",
      "  Batch 5,880  of  6,512.    Elapsed: 0:22:00.\n",
      "  Batch 5,920  of  6,512.    Elapsed: 0:22:09.\n",
      "  Batch 5,960  of  6,512.    Elapsed: 0:22:18.\n",
      "  Batch 6,000  of  6,512.    Elapsed: 0:22:27.\n",
      "  Batch 6,040  of  6,512.    Elapsed: 0:22:36.\n",
      "  Batch 6,080  of  6,512.    Elapsed: 0:22:45.\n",
      "  Batch 6,120  of  6,512.    Elapsed: 0:22:54.\n",
      "  Batch 6,160  of  6,512.    Elapsed: 0:23:03.\n",
      "  Batch 6,200  of  6,512.    Elapsed: 0:23:12.\n",
      "  Batch 6,240  of  6,512.    Elapsed: 0:23:21.\n",
      "  Batch 6,280  of  6,512.    Elapsed: 0:23:30.\n",
      "  Batch 6,320  of  6,512.    Elapsed: 0:23:39.\n",
      "  Batch 6,360  of  6,512.    Elapsed: 0:23:48.\n",
      "  Batch 6,400  of  6,512.    Elapsed: 0:23:57.\n",
      "  Batch 6,440  of  6,512.    Elapsed: 0:24:06.\n",
      "  Batch 6,480  of  6,512.    Elapsed: 0:24:15.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:24:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  6,512.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  6,512.    Elapsed: 0:00:18.\n",
      "  Batch   120  of  6,512.    Elapsed: 0:00:27.\n",
      "  Batch   160  of  6,512.    Elapsed: 0:00:36.\n",
      "  Batch   200  of  6,512.    Elapsed: 0:00:45.\n",
      "  Batch   240  of  6,512.    Elapsed: 0:00:54.\n",
      "  Batch   280  of  6,512.    Elapsed: 0:01:03.\n",
      "  Batch   320  of  6,512.    Elapsed: 0:01:12.\n",
      "  Batch   360  of  6,512.    Elapsed: 0:01:21.\n",
      "  Batch   400  of  6,512.    Elapsed: 0:01:30.\n",
      "  Batch   440  of  6,512.    Elapsed: 0:01:39.\n",
      "  Batch   480  of  6,512.    Elapsed: 0:01:48.\n",
      "  Batch   520  of  6,512.    Elapsed: 0:01:57.\n",
      "  Batch   560  of  6,512.    Elapsed: 0:02:06.\n",
      "  Batch   600  of  6,512.    Elapsed: 0:02:15.\n",
      "  Batch   640  of  6,512.    Elapsed: 0:02:24.\n",
      "  Batch   680  of  6,512.    Elapsed: 0:02:33.\n",
      "  Batch   720  of  6,512.    Elapsed: 0:02:42.\n",
      "  Batch   760  of  6,512.    Elapsed: 0:02:50.\n",
      "  Batch   800  of  6,512.    Elapsed: 0:02:59.\n",
      "  Batch   840  of  6,512.    Elapsed: 0:03:08.\n",
      "  Batch   880  of  6,512.    Elapsed: 0:03:17.\n",
      "  Batch   920  of  6,512.    Elapsed: 0:03:26.\n",
      "  Batch   960  of  6,512.    Elapsed: 0:03:35.\n",
      "  Batch 1,000  of  6,512.    Elapsed: 0:03:44.\n",
      "  Batch 1,040  of  6,512.    Elapsed: 0:03:53.\n",
      "  Batch 1,080  of  6,512.    Elapsed: 0:04:02.\n",
      "  Batch 1,120  of  6,512.    Elapsed: 0:04:11.\n",
      "  Batch 1,160  of  6,512.    Elapsed: 0:04:20.\n",
      "  Batch 1,200  of  6,512.    Elapsed: 0:04:29.\n",
      "  Batch 1,240  of  6,512.    Elapsed: 0:04:38.\n",
      "  Batch 1,280  of  6,512.    Elapsed: 0:04:47.\n",
      "  Batch 1,320  of  6,512.    Elapsed: 0:04:56.\n",
      "  Batch 1,360  of  6,512.    Elapsed: 0:05:05.\n",
      "  Batch 1,400  of  6,512.    Elapsed: 0:05:14.\n",
      "  Batch 1,440  of  6,512.    Elapsed: 0:05:23.\n",
      "  Batch 1,480  of  6,512.    Elapsed: 0:05:32.\n",
      "  Batch 1,520  of  6,512.    Elapsed: 0:05:41.\n",
      "  Batch 1,560  of  6,512.    Elapsed: 0:05:50.\n",
      "  Batch 1,600  of  6,512.    Elapsed: 0:05:59.\n",
      "  Batch 1,640  of  6,512.    Elapsed: 0:06:08.\n",
      "  Batch 1,680  of  6,512.    Elapsed: 0:06:17.\n",
      "  Batch 1,720  of  6,512.    Elapsed: 0:06:26.\n",
      "  Batch 1,760  of  6,512.    Elapsed: 0:06:35.\n",
      "  Batch 1,800  of  6,512.    Elapsed: 0:06:44.\n",
      "  Batch 1,840  of  6,512.    Elapsed: 0:06:53.\n",
      "  Batch 1,880  of  6,512.    Elapsed: 0:07:02.\n",
      "  Batch 1,920  of  6,512.    Elapsed: 0:07:11.\n",
      "  Batch 1,960  of  6,512.    Elapsed: 0:07:20.\n",
      "  Batch 2,000  of  6,512.    Elapsed: 0:07:29.\n",
      "  Batch 2,040  of  6,512.    Elapsed: 0:07:38.\n",
      "  Batch 2,080  of  6,512.    Elapsed: 0:07:47.\n",
      "  Batch 2,120  of  6,512.    Elapsed: 0:07:55.\n",
      "  Batch 2,160  of  6,512.    Elapsed: 0:08:04.\n",
      "  Batch 2,200  of  6,512.    Elapsed: 0:08:13.\n",
      "  Batch 2,240  of  6,512.    Elapsed: 0:08:22.\n",
      "  Batch 2,280  of  6,512.    Elapsed: 0:08:31.\n",
      "  Batch 2,320  of  6,512.    Elapsed: 0:08:40.\n",
      "  Batch 2,360  of  6,512.    Elapsed: 0:08:49.\n",
      "  Batch 2,400  of  6,512.    Elapsed: 0:08:58.\n",
      "  Batch 2,440  of  6,512.    Elapsed: 0:09:07.\n",
      "  Batch 2,480  of  6,512.    Elapsed: 0:09:16.\n",
      "  Batch 2,520  of  6,512.    Elapsed: 0:09:25.\n",
      "  Batch 2,560  of  6,512.    Elapsed: 0:09:34.\n",
      "  Batch 2,600  of  6,512.    Elapsed: 0:09:43.\n",
      "  Batch 2,640  of  6,512.    Elapsed: 0:09:52.\n",
      "  Batch 2,680  of  6,512.    Elapsed: 0:10:01.\n",
      "  Batch 2,720  of  6,512.    Elapsed: 0:10:10.\n",
      "  Batch 2,760  of  6,512.    Elapsed: 0:10:19.\n",
      "  Batch 2,800  of  6,512.    Elapsed: 0:10:28.\n",
      "  Batch 2,840  of  6,512.    Elapsed: 0:10:37.\n",
      "  Batch 2,880  of  6,512.    Elapsed: 0:10:46.\n",
      "  Batch 2,920  of  6,512.    Elapsed: 0:10:55.\n",
      "  Batch 2,960  of  6,512.    Elapsed: 0:11:04.\n",
      "  Batch 3,000  of  6,512.    Elapsed: 0:11:13.\n",
      "  Batch 3,040  of  6,512.    Elapsed: 0:11:22.\n",
      "  Batch 3,080  of  6,512.    Elapsed: 0:11:31.\n",
      "  Batch 3,120  of  6,512.    Elapsed: 0:11:40.\n",
      "  Batch 3,160  of  6,512.    Elapsed: 0:11:49.\n",
      "  Batch 3,200  of  6,512.    Elapsed: 0:11:58.\n",
      "  Batch 3,240  of  6,512.    Elapsed: 0:12:07.\n",
      "  Batch 3,280  of  6,512.    Elapsed: 0:12:16.\n",
      "  Batch 3,320  of  6,512.    Elapsed: 0:12:25.\n",
      "  Batch 3,360  of  6,512.    Elapsed: 0:12:34.\n",
      "  Batch 3,400  of  6,512.    Elapsed: 0:12:43.\n",
      "  Batch 3,440  of  6,512.    Elapsed: 0:12:52.\n",
      "  Batch 3,480  of  6,512.    Elapsed: 0:13:00.\n",
      "  Batch 3,520  of  6,512.    Elapsed: 0:13:09.\n",
      "  Batch 3,560  of  6,512.    Elapsed: 0:13:18.\n",
      "  Batch 3,600  of  6,512.    Elapsed: 0:13:27.\n",
      "  Batch 3,640  of  6,512.    Elapsed: 0:13:36.\n",
      "  Batch 3,680  of  6,512.    Elapsed: 0:13:45.\n",
      "  Batch 3,720  of  6,512.    Elapsed: 0:13:54.\n",
      "  Batch 3,760  of  6,512.    Elapsed: 0:14:03.\n",
      "  Batch 3,800  of  6,512.    Elapsed: 0:14:12.\n",
      "  Batch 3,840  of  6,512.    Elapsed: 0:14:21.\n",
      "  Batch 3,880  of  6,512.    Elapsed: 0:14:30.\n",
      "  Batch 3,920  of  6,512.    Elapsed: 0:14:39.\n",
      "  Batch 3,960  of  6,512.    Elapsed: 0:14:48.\n",
      "  Batch 4,000  of  6,512.    Elapsed: 0:14:57.\n",
      "  Batch 4,040  of  6,512.    Elapsed: 0:15:06.\n",
      "  Batch 4,080  of  6,512.    Elapsed: 0:15:15.\n",
      "  Batch 4,120  of  6,512.    Elapsed: 0:15:24.\n",
      "  Batch 4,160  of  6,512.    Elapsed: 0:15:33.\n",
      "  Batch 4,200  of  6,512.    Elapsed: 0:15:42.\n",
      "  Batch 4,240  of  6,512.    Elapsed: 0:15:51.\n",
      "  Batch 4,280  of  6,512.    Elapsed: 0:16:00.\n",
      "  Batch 4,320  of  6,512.    Elapsed: 0:16:09.\n",
      "  Batch 4,360  of  6,512.    Elapsed: 0:16:18.\n",
      "  Batch 4,400  of  6,512.    Elapsed: 0:16:27.\n",
      "  Batch 4,440  of  6,512.    Elapsed: 0:16:36.\n",
      "  Batch 4,480  of  6,512.    Elapsed: 0:16:45.\n",
      "  Batch 4,520  of  6,512.    Elapsed: 0:16:54.\n",
      "  Batch 4,560  of  6,512.    Elapsed: 0:17:03.\n",
      "  Batch 4,600  of  6,512.    Elapsed: 0:17:12.\n",
      "  Batch 4,640  of  6,512.    Elapsed: 0:17:21.\n",
      "  Batch 4,680  of  6,512.    Elapsed: 0:17:30.\n",
      "  Batch 4,720  of  6,512.    Elapsed: 0:17:39.\n",
      "  Batch 4,760  of  6,512.    Elapsed: 0:17:48.\n",
      "  Batch 4,800  of  6,512.    Elapsed: 0:17:57.\n",
      "  Batch 4,840  of  6,512.    Elapsed: 0:18:05.\n",
      "  Batch 4,880  of  6,512.    Elapsed: 0:18:14.\n",
      "  Batch 4,920  of  6,512.    Elapsed: 0:18:23.\n",
      "  Batch 4,960  of  6,512.    Elapsed: 0:18:32.\n",
      "  Batch 5,000  of  6,512.    Elapsed: 0:18:41.\n",
      "  Batch 5,040  of  6,512.    Elapsed: 0:18:50.\n",
      "  Batch 5,080  of  6,512.    Elapsed: 0:18:59.\n",
      "  Batch 5,120  of  6,512.    Elapsed: 0:19:08.\n",
      "  Batch 5,160  of  6,512.    Elapsed: 0:19:17.\n",
      "  Batch 5,200  of  6,512.    Elapsed: 0:19:26.\n",
      "  Batch 5,240  of  6,512.    Elapsed: 0:19:35.\n",
      "  Batch 5,280  of  6,512.    Elapsed: 0:19:44.\n",
      "  Batch 5,320  of  6,512.    Elapsed: 0:19:53.\n",
      "  Batch 5,360  of  6,512.    Elapsed: 0:20:02.\n",
      "  Batch 5,400  of  6,512.    Elapsed: 0:20:11.\n",
      "  Batch 5,440  of  6,512.    Elapsed: 0:20:20.\n",
      "  Batch 5,480  of  6,512.    Elapsed: 0:20:29.\n",
      "  Batch 5,520  of  6,512.    Elapsed: 0:20:38.\n",
      "  Batch 5,560  of  6,512.    Elapsed: 0:20:47.\n",
      "  Batch 5,600  of  6,512.    Elapsed: 0:20:56.\n",
      "  Batch 5,640  of  6,512.    Elapsed: 0:21:05.\n",
      "  Batch 5,680  of  6,512.    Elapsed: 0:21:14.\n",
      "  Batch 5,720  of  6,512.    Elapsed: 0:21:23.\n",
      "  Batch 5,760  of  6,512.    Elapsed: 0:21:32.\n",
      "  Batch 5,800  of  6,512.    Elapsed: 0:21:41.\n",
      "  Batch 5,840  of  6,512.    Elapsed: 0:21:50.\n",
      "  Batch 5,880  of  6,512.    Elapsed: 0:21:59.\n",
      "  Batch 5,920  of  6,512.    Elapsed: 0:22:08.\n",
      "  Batch 5,960  of  6,512.    Elapsed: 0:22:17.\n",
      "  Batch 6,000  of  6,512.    Elapsed: 0:22:26.\n",
      "  Batch 6,040  of  6,512.    Elapsed: 0:22:35.\n",
      "  Batch 6,080  of  6,512.    Elapsed: 0:22:44.\n",
      "  Batch 6,120  of  6,512.    Elapsed: 0:22:53.\n",
      "  Batch 6,160  of  6,512.    Elapsed: 0:23:02.\n",
      "  Batch 6,200  of  6,512.    Elapsed: 0:23:11.\n",
      "  Batch 6,240  of  6,512.    Elapsed: 0:23:19.\n",
      "  Batch 6,280  of  6,512.    Elapsed: 0:23:28.\n",
      "  Batch 6,320  of  6,512.    Elapsed: 0:23:37.\n",
      "  Batch 6,360  of  6,512.    Elapsed: 0:23:46.\n",
      "  Batch 6,400  of  6,512.    Elapsed: 0:23:55.\n",
      "  Batch 6,440  of  6,512.    Elapsed: 0:24:04.\n",
      "  Batch 6,480  of  6,512.    Elapsed: 0:24:13.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:24:21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.37\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  6,512.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  6,512.    Elapsed: 0:00:18.\n",
      "  Batch   120  of  6,512.    Elapsed: 0:00:27.\n",
      "  Batch   160  of  6,512.    Elapsed: 0:00:36.\n",
      "  Batch   200  of  6,512.    Elapsed: 0:00:45.\n",
      "  Batch   240  of  6,512.    Elapsed: 0:00:54.\n",
      "  Batch   280  of  6,512.    Elapsed: 0:01:03.\n",
      "  Batch   320  of  6,512.    Elapsed: 0:01:12.\n",
      "  Batch   360  of  6,512.    Elapsed: 0:01:21.\n",
      "  Batch   400  of  6,512.    Elapsed: 0:01:30.\n",
      "  Batch   440  of  6,512.    Elapsed: 0:01:39.\n",
      "  Batch   480  of  6,512.    Elapsed: 0:01:48.\n",
      "  Batch   520  of  6,512.    Elapsed: 0:01:57.\n",
      "  Batch   560  of  6,512.    Elapsed: 0:02:06.\n",
      "  Batch   600  of  6,512.    Elapsed: 0:02:15.\n",
      "  Batch   640  of  6,512.    Elapsed: 0:02:24.\n",
      "  Batch   680  of  6,512.    Elapsed: 0:02:33.\n",
      "  Batch   720  of  6,512.    Elapsed: 0:02:42.\n",
      "  Batch   760  of  6,512.    Elapsed: 0:02:51.\n",
      "  Batch   800  of  6,512.    Elapsed: 0:02:59.\n",
      "  Batch   840  of  6,512.    Elapsed: 0:03:08.\n",
      "  Batch   880  of  6,512.    Elapsed: 0:03:17.\n",
      "  Batch   920  of  6,512.    Elapsed: 0:03:26.\n",
      "  Batch   960  of  6,512.    Elapsed: 0:03:35.\n",
      "  Batch 1,000  of  6,512.    Elapsed: 0:03:44.\n",
      "  Batch 1,040  of  6,512.    Elapsed: 0:03:53.\n",
      "  Batch 1,080  of  6,512.    Elapsed: 0:04:02.\n",
      "  Batch 1,120  of  6,512.    Elapsed: 0:04:11.\n",
      "  Batch 1,160  of  6,512.    Elapsed: 0:04:20.\n",
      "  Batch 1,200  of  6,512.    Elapsed: 0:04:29.\n",
      "  Batch 1,240  of  6,512.    Elapsed: 0:04:38.\n",
      "  Batch 1,280  of  6,512.    Elapsed: 0:04:47.\n",
      "  Batch 1,320  of  6,512.    Elapsed: 0:04:56.\n",
      "  Batch 1,360  of  6,512.    Elapsed: 0:05:05.\n",
      "  Batch 1,400  of  6,512.    Elapsed: 0:05:14.\n",
      "  Batch 1,440  of  6,512.    Elapsed: 0:05:23.\n",
      "  Batch 1,480  of  6,512.    Elapsed: 0:05:32.\n",
      "  Batch 1,520  of  6,512.    Elapsed: 0:05:41.\n",
      "  Batch 1,560  of  6,512.    Elapsed: 0:05:50.\n",
      "  Batch 1,600  of  6,512.    Elapsed: 0:05:59.\n",
      "  Batch 1,640  of  6,512.    Elapsed: 0:06:08.\n",
      "  Batch 1,680  of  6,512.    Elapsed: 0:06:17.\n",
      "  Batch 1,720  of  6,512.    Elapsed: 0:06:26.\n",
      "  Batch 1,760  of  6,512.    Elapsed: 0:06:35.\n",
      "  Batch 1,800  of  6,512.    Elapsed: 0:06:44.\n",
      "  Batch 1,840  of  6,512.    Elapsed: 0:06:53.\n",
      "  Batch 1,880  of  6,512.    Elapsed: 0:07:02.\n",
      "  Batch 1,920  of  6,512.    Elapsed: 0:07:11.\n",
      "  Batch 1,960  of  6,512.    Elapsed: 0:07:20.\n",
      "  Batch 2,000  of  6,512.    Elapsed: 0:07:29.\n",
      "  Batch 2,040  of  6,512.    Elapsed: 0:07:38.\n",
      "  Batch 2,080  of  6,512.    Elapsed: 0:07:47.\n",
      "  Batch 2,120  of  6,512.    Elapsed: 0:07:56.\n",
      "  Batch 2,160  of  6,512.    Elapsed: 0:08:05.\n",
      "  Batch 2,200  of  6,512.    Elapsed: 0:08:13.\n",
      "  Batch 2,240  of  6,512.    Elapsed: 0:08:22.\n",
      "  Batch 2,280  of  6,512.    Elapsed: 0:08:31.\n",
      "  Batch 2,320  of  6,512.    Elapsed: 0:08:40.\n",
      "  Batch 2,360  of  6,512.    Elapsed: 0:08:49.\n",
      "  Batch 2,400  of  6,512.    Elapsed: 0:08:58.\n",
      "  Batch 2,440  of  6,512.    Elapsed: 0:09:07.\n",
      "  Batch 2,480  of  6,512.    Elapsed: 0:09:16.\n",
      "  Batch 2,520  of  6,512.    Elapsed: 0:09:25.\n",
      "  Batch 2,560  of  6,512.    Elapsed: 0:09:34.\n",
      "  Batch 2,600  of  6,512.    Elapsed: 0:09:43.\n",
      "  Batch 2,640  of  6,512.    Elapsed: 0:09:52.\n",
      "  Batch 2,680  of  6,512.    Elapsed: 0:10:01.\n",
      "  Batch 2,720  of  6,512.    Elapsed: 0:10:10.\n",
      "  Batch 2,760  of  6,512.    Elapsed: 0:10:19.\n",
      "  Batch 2,800  of  6,512.    Elapsed: 0:10:28.\n",
      "  Batch 2,840  of  6,512.    Elapsed: 0:10:37.\n",
      "  Batch 2,880  of  6,512.    Elapsed: 0:10:46.\n",
      "  Batch 2,920  of  6,512.    Elapsed: 0:10:55.\n",
      "  Batch 2,960  of  6,512.    Elapsed: 0:11:04.\n",
      "  Batch 3,000  of  6,512.    Elapsed: 0:11:13.\n",
      "  Batch 3,040  of  6,512.    Elapsed: 0:11:22.\n",
      "  Batch 3,080  of  6,512.    Elapsed: 0:11:31.\n",
      "  Batch 3,120  of  6,512.    Elapsed: 0:11:40.\n",
      "  Batch 3,160  of  6,512.    Elapsed: 0:11:49.\n",
      "  Batch 3,200  of  6,512.    Elapsed: 0:11:58.\n",
      "  Batch 3,240  of  6,512.    Elapsed: 0:12:07.\n",
      "  Batch 3,280  of  6,512.    Elapsed: 0:12:16.\n",
      "  Batch 3,320  of  6,512.    Elapsed: 0:12:25.\n",
      "  Batch 3,360  of  6,512.    Elapsed: 0:12:34.\n",
      "  Batch 3,400  of  6,512.    Elapsed: 0:12:43.\n",
      "  Batch 3,440  of  6,512.    Elapsed: 0:12:52.\n",
      "  Batch 3,480  of  6,512.    Elapsed: 0:13:01.\n",
      "  Batch 3,520  of  6,512.    Elapsed: 0:13:10.\n",
      "  Batch 3,560  of  6,512.    Elapsed: 0:13:19.\n",
      "  Batch 3,600  of  6,512.    Elapsed: 0:13:27.\n",
      "  Batch 3,640  of  6,512.    Elapsed: 0:13:36.\n",
      "  Batch 3,680  of  6,512.    Elapsed: 0:13:45.\n",
      "  Batch 3,720  of  6,512.    Elapsed: 0:13:54.\n",
      "  Batch 3,760  of  6,512.    Elapsed: 0:14:03.\n",
      "  Batch 3,800  of  6,512.    Elapsed: 0:14:12.\n",
      "  Batch 3,840  of  6,512.    Elapsed: 0:14:21.\n",
      "  Batch 3,880  of  6,512.    Elapsed: 0:14:30.\n",
      "  Batch 3,920  of  6,512.    Elapsed: 0:14:39.\n",
      "  Batch 3,960  of  6,512.    Elapsed: 0:14:48.\n",
      "  Batch 4,000  of  6,512.    Elapsed: 0:14:57.\n",
      "  Batch 4,040  of  6,512.    Elapsed: 0:15:06.\n",
      "  Batch 4,080  of  6,512.    Elapsed: 0:15:15.\n",
      "  Batch 4,120  of  6,512.    Elapsed: 0:15:24.\n",
      "  Batch 4,160  of  6,512.    Elapsed: 0:15:33.\n",
      "  Batch 4,200  of  6,512.    Elapsed: 0:15:42.\n",
      "  Batch 4,240  of  6,512.    Elapsed: 0:15:51.\n",
      "  Batch 4,280  of  6,512.    Elapsed: 0:16:00.\n",
      "  Batch 4,320  of  6,512.    Elapsed: 0:16:09.\n",
      "  Batch 4,360  of  6,512.    Elapsed: 0:16:18.\n",
      "  Batch 4,400  of  6,512.    Elapsed: 0:16:27.\n",
      "  Batch 4,440  of  6,512.    Elapsed: 0:16:36.\n",
      "  Batch 4,480  of  6,512.    Elapsed: 0:16:45.\n",
      "  Batch 4,520  of  6,512.    Elapsed: 0:16:55.\n",
      "  Batch 4,560  of  6,512.    Elapsed: 0:17:05.\n",
      "  Batch 4,600  of  6,512.    Elapsed: 0:17:14.\n",
      "  Batch 4,640  of  6,512.    Elapsed: 0:17:24.\n",
      "  Batch 4,680  of  6,512.    Elapsed: 0:17:33.\n",
      "  Batch 4,720  of  6,512.    Elapsed: 0:17:42.\n",
      "  Batch 4,760  of  6,512.    Elapsed: 0:17:51.\n",
      "  Batch 4,800  of  6,512.    Elapsed: 0:18:00.\n",
      "  Batch 4,840  of  6,512.    Elapsed: 0:18:09.\n",
      "  Batch 4,880  of  6,512.    Elapsed: 0:18:18.\n",
      "  Batch 4,920  of  6,512.    Elapsed: 0:18:27.\n",
      "  Batch 4,960  of  6,512.    Elapsed: 0:18:36.\n",
      "  Batch 5,000  of  6,512.    Elapsed: 0:18:45.\n",
      "  Batch 5,040  of  6,512.    Elapsed: 0:18:54.\n",
      "  Batch 5,080  of  6,512.    Elapsed: 0:19:03.\n",
      "  Batch 5,120  of  6,512.    Elapsed: 0:19:12.\n",
      "  Batch 5,160  of  6,512.    Elapsed: 0:19:21.\n",
      "  Batch 5,200  of  6,512.    Elapsed: 0:19:30.\n",
      "  Batch 5,240  of  6,512.    Elapsed: 0:19:39.\n",
      "  Batch 5,280  of  6,512.    Elapsed: 0:19:48.\n",
      "  Batch 5,320  of  6,512.    Elapsed: 0:19:57.\n",
      "  Batch 5,360  of  6,512.    Elapsed: 0:20:06.\n",
      "  Batch 5,400  of  6,512.    Elapsed: 0:20:15.\n",
      "  Batch 5,440  of  6,512.    Elapsed: 0:20:24.\n",
      "  Batch 5,480  of  6,512.    Elapsed: 0:20:33.\n",
      "  Batch 5,520  of  6,512.    Elapsed: 0:20:42.\n",
      "  Batch 5,560  of  6,512.    Elapsed: 0:20:51.\n",
      "  Batch 5,600  of  6,512.    Elapsed: 0:21:00.\n",
      "  Batch 5,640  of  6,512.    Elapsed: 0:21:09.\n",
      "  Batch 5,680  of  6,512.    Elapsed: 0:21:18.\n",
      "  Batch 5,720  of  6,512.    Elapsed: 0:21:27.\n",
      "  Batch 5,760  of  6,512.    Elapsed: 0:21:36.\n",
      "  Batch 5,800  of  6,512.    Elapsed: 0:21:45.\n",
      "  Batch 5,840  of  6,512.    Elapsed: 0:21:54.\n",
      "  Batch 5,880  of  6,512.    Elapsed: 0:22:03.\n",
      "  Batch 5,920  of  6,512.    Elapsed: 0:22:12.\n",
      "  Batch 5,960  of  6,512.    Elapsed: 0:22:21.\n",
      "  Batch 6,000  of  6,512.    Elapsed: 0:22:30.\n",
      "  Batch 6,040  of  6,512.    Elapsed: 0:22:39.\n",
      "  Batch 6,080  of  6,512.    Elapsed: 0:22:48.\n",
      "  Batch 6,120  of  6,512.    Elapsed: 0:22:57.\n",
      "  Batch 6,160  of  6,512.    Elapsed: 0:23:06.\n",
      "  Batch 6,200  of  6,512.    Elapsed: 0:23:15.\n",
      "  Batch 6,240  of  6,512.    Elapsed: 0:23:24.\n",
      "  Batch 6,280  of  6,512.    Elapsed: 0:23:33.\n",
      "  Batch 6,320  of  6,512.    Elapsed: 0:23:42.\n",
      "  Batch 6,360  of  6,512.    Elapsed: 0:23:51.\n",
      "  Batch 6,400  of  6,512.    Elapsed: 0:24:00.\n",
      "  Batch 6,440  of  6,512.    Elapsed: 0:24:09.\n",
      "  Batch 6,480  of  6,512.    Elapsed: 0:24:18.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:24:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60\n",
      "  Validation Loss: 1.06\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:37:41 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "#  code from Chris McCormick site:\n",
    "#  source:\n",
    "#\n",
    "#  https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "\n",
    "total_train_loss = 0\n",
    "\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "Let's view the summary of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "6O_NbXFGMukX",
    "outputId": "a9e51eda-5eae-4800-87d5-8d016ff25bb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0:24:32</td>\n",
       "      <td>0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0:24:22</td>\n",
       "      <td>0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0:24:21</td>\n",
       "      <td>0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0:24:26</td>\n",
       "      <td>0:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.49         0.27            0.8       0:24:32         0:00:00\n",
       "2               0.38         0.26            0.8       0:24:22         0:00:00\n",
       "3               0.26         0.37            0.8       0:24:21         0:00:00\n",
       "4               0.16         1.06            0.6       0:24:26         0:00:00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### print out validation, training loss / and times here:\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "68xreA9JAmG5",
    "outputId": "70b8500d-7efc-4c99-de1f-05e8795e6298"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training Loss\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation Loss\")\n",
    "plt.plot(df_stats['Valid. Accur.'], 'r-o', label=\"Validation Accuracy\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss , Accuracy\")\n",
    "plt.legend()\n",
    "plt.xticks(range(1,epochs+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAN0LZBOOPVh",
    "outputId": "7385ca3f-72d5-45f0-bbfe-5056c2f62c4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-2000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1997</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>119087</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>119088</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>119089</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>119090</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>119091</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id original_text  label\n",
       "0            0         -2011    NaN\n",
       "1            1         -2011    NaN\n",
       "2            2         -2000    NaN\n",
       "3            3         -1997    NaN\n",
       "4            4         1.636    NaN\n",
       "...        ...           ...    ...\n",
       "119087  119087        #NAME?    NaN\n",
       "119088  119088        #NAME?    NaN\n",
       "119089  119089        #NAME?    NaN\n",
       "119090  119090        #NAME?    NaN\n",
       "119091  119091        #NAME?    NaN\n",
       "\n",
       "[119092 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-2000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1997</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>119087</td>\n",
       "      <td>#name?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>119088</td>\n",
       "      <td>#name?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>119089</td>\n",
       "      <td>#name?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>119090</td>\n",
       "      <td>#name?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>119091</td>\n",
       "      <td>#name?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id text_clean  label\n",
       "0            0      -2011    NaN\n",
       "1            1      -2011    NaN\n",
       "2            2      -2000    NaN\n",
       "3            3      -1997    NaN\n",
       "4            4      1.636    NaN\n",
       "...        ...        ...    ...\n",
       "119087  119087     #name?    NaN\n",
       "119088  119088     #name?    NaN\n",
       "119089  119089     #name?    NaN\n",
       "119090  119090     #name?    NaN\n",
       "119091  119091     #name?    NaN\n",
       "\n",
       "[119092 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119092\n",
      "119092\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>#name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>#name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>#name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>#name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>#name?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_clean\n",
       "0           -2011\n",
       "1           -2011\n",
       "2           -2000\n",
       "3           -1997\n",
       "4           1.636\n",
       "...           ...\n",
       "119087     #name?\n",
       "119088     #name?\n",
       "119089     #name?\n",
       "119090     #name?\n",
       "119091     #name?\n",
       "\n",
       "[119092 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 119,092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierrel/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "# update here set sentences and labels = 696 data instead here:\n",
    "# test file name:  TEST_data_export_2_features.csv\n",
    "# modified by pierrel\n",
    "\n",
    "var_WikiText_TEST_clean_df = pd.read_csv('data/WikiLarge_Test.csv')\n",
    "\n",
    "display(var_WikiText_TEST_clean_df)\n",
    "\n",
    "var_WikiText_TEST_clean_df['original_text'] = var_WikiText_TEST_clean_df['original_text'].str.lower()\n",
    "var_WikiText_TEST_clean_df.columns = ['id','text_clean','label']\n",
    "\n",
    "\n",
    "display(var_WikiText_TEST_clean_df)\n",
    "\n",
    "#var_WikiText_TEST_df = var_WikiText_TEST_clean_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#var_WikiText_TEST_clean_df = pd.read_csv('TEST_data_export_2_features.csv')\n",
    "\n",
    "var_WikiText_TEST_df = var_WikiText_TEST_clean_df[['text_clean']]\n",
    "\n",
    "sentences = var_WikiText_TEST_df.text_clean.values\n",
    "#labels = var_WikiText_TEST_df.label.values\n",
    "var_list_zeros = listofzeros = [0] * len(var_WikiText_TEST_df)\n",
    "print(len(var_WikiText_TEST_df))\n",
    "print(len(var_list_zeros))\n",
    "labels = var_list_zeros\n",
    "\n",
    "display(var_WikiText_TEST_df)\n",
    "display()\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(var_WikiText_TEST_df.shape[0]))\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "#  code from Chris McCormick site:\n",
    "#  source:\n",
    "#\n",
    "#  https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences. (was 64 originally here)\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "e35f0a6e-72c5-4bd0-9c4b-dcec9ef5059d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 119,092 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWcy0X1hirdx",
    "outputId": "ef5e6753-c244-406a-8141-5078d71b04ee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCYZa1lQ8Jn8",
    "outputId": "b4650298-0e35-4ed8-be13-83f074a617ed"
   },
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3096459/2977212491.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  var_WikiText_TEST_df['predictions'] = flat_predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119087</th>\n",
       "      <td>119087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119088</th>\n",
       "      <td>119088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119089</th>\n",
       "      <td>119089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119090</th>\n",
       "      <td>119090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119091</th>\n",
       "      <td>119091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label\n",
       "0            0      0\n",
       "1            1      0\n",
       "2            2      0\n",
       "3            3      0\n",
       "4            4      1\n",
       "...        ...    ...\n",
       "119087  119087      0\n",
       "119088  119088      0\n",
       "119089  119089      0\n",
       "119090  119090      0\n",
       "119091  119091      0\n",
       "\n",
       "[119092 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### export flat predictions here\n",
    "print(type(flat_predictions))\n",
    "\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "###################################################################################\n",
    "#   696 - pierrel - umich - update code here to export flat predictions to csv:\n",
    "\n",
    "var_predictions_696_df = pd.DataFrame(flat_predictions)\n",
    "\n",
    "var_WikiText_TEST_df['predictions'] = flat_predictions\n",
    "\n",
    "var_WikiText_TEST_df = var_WikiText_TEST_df.reset_index()\n",
    "\n",
    "#display(var_WikiText_TEST_df)\n",
    "\n",
    "var_WikiText_TEST_df_output = var_WikiText_TEST_df[['index','predictions']]\n",
    "var_WikiText_TEST_df_output.columns = ['id','label']\n",
    "\n",
    "display(var_WikiText_TEST_df_output)\n",
    "\n",
    "\n",
    "#  export test dataframe here \n",
    "var_WikiText_TEST_df_output.to_csv('TEST_BERT_data_export_4_epochs_BERT_base_100k_tests_Team5.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "#display(var_WikiText_TEST_df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60872"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_WikiText_TEST_df_output['label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('var_WikiText_Train_clean_df', 77031000),\n",
       " ('var_WikiText_Train_df', 77031000),\n",
       " ('var_WikiText_TEST_clean_df', 22984587),\n",
       " ('var_WikiText_TEST_df', 22984587),\n",
       " ('var_WikiText_TEST_df_output', 1905616),\n",
       " ('var_predictions_696_df', 952880),\n",
       " ('flat_predictions', 952848),\n",
       " ('flat_true_labels', 952848),\n",
       " ('listofzeros', 952792),\n",
       " ('var_list_zeros', 952792),\n",
       " ('predictions', 33048),\n",
       " ('true_labels', 33048),\n",
       " ('BertTokenizer', 2008),\n",
       " ('params', 1912),\n",
       " ('BertForSequenceClassification', 1472),\n",
       " ('DataLoader', 1472),\n",
       " ('AdamW', 1064),\n",
       " ('RandomSampler', 1064),\n",
       " ('SequentialSampler', 1064),\n",
       " ('TensorDataset', 1064),\n",
       " ('BertConfig', 896),\n",
       " ('df_stats', 656),\n",
       " ('result', 392),\n",
       " ('flat_accuracy', 136),\n",
       " ('format_time', 136),\n",
       " ('get_linear_schedule_with_warmup', 136),\n",
       " ('random_split', 136),\n",
       " ('logits', 128),\n",
       " ('label_ids', 112),\n",
       " ('sentences', 112),\n",
       " ('training_stats', 88),\n",
       " ('attention_masks', 72),\n",
       " ('b_input_ids', 72),\n",
       " ('b_input_mask', 72),\n",
       " ('b_labels', 72),\n",
       " ('input_ids', 72),\n",
       " ('labels', 72),\n",
       " ('loss', 72),\n",
       " ('np', 72),\n",
       " ('pd', 72),\n",
       " ('plt', 72),\n",
       " ('sns', 72),\n",
       " ('tf', 72),\n",
       " ('batch', 64),\n",
       " ('elapsed', 56),\n",
       " ('p', 56),\n",
       " ('training_time', 56),\n",
       " ('validation_time', 56),\n",
       " ('sent', 55),\n",
       " ('device_name', 49),\n",
       " ('dataset', 48),\n",
       " ('encoded_dict', 48),\n",
       " ('model', 48),\n",
       " ('optimizer', 48),\n",
       " ('prediction_data', 48),\n",
       " ('prediction_dataloader', 48),\n",
       " ('prediction_sampler', 48),\n",
       " ('scheduler', 48),\n",
       " ('tokenizer', 48),\n",
       " ('train_dataloader', 48),\n",
       " ('train_dataset', 48),\n",
       " ('val_dataset', 48),\n",
       " ('validation_dataloader', 48),\n",
       " ('avg_val_accuracy', 32),\n",
       " ('total_eval_accuracy', 32),\n",
       " ('batch_size', 28),\n",
       " ('epoch_i', 28),\n",
       " ('epochs', 28),\n",
       " ('seed_val', 28),\n",
       " ('step', 28),\n",
       " ('total_steps', 28),\n",
       " ('train_size', 28),\n",
       " ('val_size', 28),\n",
       " ('avg_train_loss', 24),\n",
       " ('avg_val_loss', 24),\n",
       " ('device', 24),\n",
       " ('nb_eval_steps', 24),\n",
       " ('t0', 24),\n",
       " ('total_eval_loss', 24),\n",
       " ('total_t0', 24),\n",
       " ('total_train_loss', 24)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### check memory here of objects:\n",
    "####\n",
    "#### reference from:  https://stackoverflow.com/questions/40993626/list-memory-usage-in-ipython-and-jupyter\n",
    "\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is using =  63.720602851827934  %\n"
     ]
    }
   ],
   "source": [
    "var_memory = torch.cuda.mem_get_info()\n",
    "\n",
    "print(\"GPU is using = \", (var_memory[0] / var_memory[1])*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 6.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "# Getting all memory using os.popen()\n",
    "total_memory, used_memory, free_memory = map(\n",
    "    int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    " \n",
    "# Memory usage\n",
    "print(\"RAM memory % used:\", round((used_memory/total_memory) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fe5b1d0540240a8a8426352c24b2887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1058e0b5baa248faa60c1ad146d10bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1296a3d754b344a482a03e5af84e805e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8874fec8a404ae89a38fd2ecbb357cf",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2755b9838bae408ca8cf667ad9d501fc",
      "value": 433
     }
    },
    "1c2b0ede959142fc89bf07a9c88df638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ca9359e6c44232a1346e6f2ab7e48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fe5b1d0540240a8a8426352c24b2887",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c7dec7b1e804c2195f6e60fb3c1d18e",
      "value": 440473133
     }
    },
    "2755b9838bae408ca8cf667ad9d501fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "375cc635389c4ddb9bf2aa443df58bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "440da34c72344cb08e4a1ee5de7049ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "472198d5b6a748b3a81f9364fd1fa711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1e27aff6f04fec8268d951e46b1e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7dec7b1e804c2195f6e60fb3c1d18e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6f132d7bb83d41b6847df0d0ec0a1b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_978c24b18b594eaf8ca47730a88eefb9",
      "placeholder": "​",
      "style": "IPY_MODEL_a7bdbedc75de4f77b45f1389c2ea0abc",
      "value": " 433/433 [00:00&lt;00:00, 2.02kB/s]"
     }
    },
    "82ddfcea0e4c4e5a86cf6eca8585be8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c76faadf2f4415393c6f0a805f0d72b",
       "IPY_MODEL_e0bb735fda99434a90380e7fc664212d"
      ],
      "layout": "IPY_MODEL_8a256ba4a19e4ec98fe3c3c99fba4daa"
     }
    },
    "8a256ba4a19e4ec98fe3c3c99fba4daa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c76faadf2f4415393c6f0a805f0d72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1058e0b5baa248faa60c1ad146d10bf7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdb78e75309f4bc09366533331e72431",
      "value": 231508
     }
    },
    "978c24b18b594eaf8ca47730a88eefb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bdbedc75de4f77b45f1389c2ea0abc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf9dfa1ff3e642fbb74c5146d21044c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1296a3d754b344a482a03e5af84e805e",
       "IPY_MODEL_6f132d7bb83d41b6847df0d0ec0a1b92"
      ],
      "layout": "IPY_MODEL_1c2b0ede959142fc89bf07a9c88df638"
     }
    },
    "cdb78e75309f4bc09366533331e72431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cea84f9c3db641acb98314028b305514": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d689bc8d488a4dc09c393b4fc9747bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_440da34c72344cb08e4a1ee5de7049ee",
      "placeholder": "​",
      "style": "IPY_MODEL_4b1e27aff6f04fec8268d951e46b1e63",
      "value": " 440M/440M [00:07&lt;00:00, 55.5MB/s]"
     }
    },
    "e0bb735fda99434a90380e7fc664212d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_472198d5b6a748b3a81f9364fd1fa711",
      "placeholder": "​",
      "style": "IPY_MODEL_375cc635389c4ddb9bf2aa443df58bae",
      "value": " 232k/232k [00:00&lt;00:00, 616kB/s]"
     }
    },
    "f8874fec8a404ae89a38fd2ecbb357cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe254c3bcc08402eb506f0e98f5673a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23ca9359e6c44232a1346e6f2ab7e48c",
       "IPY_MODEL_d689bc8d488a4dc09c393b4fc9747bcb"
      ],
      "layout": "IPY_MODEL_cea84f9c3db641acb98314028b305514"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
